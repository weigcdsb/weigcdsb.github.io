---
layout: post
title: "Diffusion Mumbling"
date: 2025-09-21
---

This post collects some of my recent thoughts and readings on diffusion models. As things are not formal, I simply call them "diffusion mumbling", as they are literally some sweet mumblings. While many of the notes and opinions are very personal (it is my blog, after all, yay), I’ll try to keep them clear and reasonable. A more structured summary can be found in these [slides](/assets/slides/Diffusion Mumbling_09212025.pdf). I won’t repeat everything here: partly because I’m lazy, partly because this is just a personal note. In short, there are two major sections (weighting and ELBO/ likelihood) and two minor sections (prior and sampling), which I’ll briefly outline below. Many ideas are inspired from these two fantastic blogs([1](https://d2jud02ci9yv69.cloudfront.net/2025-04-28-diffusion-flow-173/blog/diffusion-flow/) and [2](https://sander.ai/2024/06/14/noise-schedules.html?utm_source=chatgpt.com#overview)). For paper references, see the slides for details.

# 1. Weighting in the Unifying Framework

When we put the loss function of a variety of diffusion models (e.g. DDPM, score matching and flow matching) together, they look strikingly similar. Therefore, a natural idea is to find a framework to unify them. As far as I know, there are at least two papers on this: the ["stochastic interpolant"](https://arxiv.org/abs/2303.08797) paper and ["understanding diffusion objective"](https://openreview.net/pdf?id=NnMEadcdyD) paper. I read the "stochastic interpolant" first, which unifies things mainly from ODE/ SDE perspective. This work is great, comprehesive, and mathematically sophisticated. But, at least for me, the inspiration it provided is relatively limited. However, when I later read the "understanding diffusion" one, which frames things mainly from the loss and likelihood perspective, I would say it is breathtaking! For anyone interested in diffusion models, I would recommend it as a must have to read.

OK, here’s my very personal take. For diffusion models, both the ODE/SDE view and the loss/prob view are important. The ODE/SDE side helps us dig into technical details and inspires new model designs, while the loss/prob side gives a higher-level understanding. Why do I say this? Because diffusion models are basically stacked (or hierarchical) VAEs, and the ODE/SDE is what “glues” the different layers together. To really get the big picture, we usually have to go back to the original purpose and motivation, which is all about VAE/ loss/ prob. But to actually make diffusion models work, we need clever ways to design the “junctions” between layers. That’s the hard part, and the design really depends on the data. This is exactly where ODE/SDE perspectives kick in.

In the "understanding" paper, they first unify almost all diffusion models with 1) Gaussian source and 2) linear noise schedule/ interpolation ($z_t = \alpha_t x + \sigma_t \epsilon$, where $x$ is data and $\epsilon ~ N(0, I)$ is the Gaussian noise). After some reparametrizations/ rearrangements, everything is reduced to weighting of the loss (if we seperate the noise schedule out, which is essentially importance sampling). This is fantastic and very inspiring! When we spot on the weighting for different diffusion models, we can see that:
1. The diffusion models put a large weight on low SNR, i.e., closer to noise. This is why the diffusion models are fantastic in perceptual data (e.g., image, video, sound): we human are mainly sensitive low-frequency/ low-resolution part of perceptual data, and diffusion model just cater us on this. Basically, the model has limited capacity, and it cannot do perfectly for both high and low frequency domain. The diffusion models just sacrifice the high frequency part to make things "look"/ "sound" perfect.
2. The flow matching (FM) is very efficient, since it aggresively put almost all the weight to the low-SNR region. And this is why we usually see FM almost complete generation in the first few steps! FM is too aggresive on low-SNR part, and we need to let it slow down. Therefore, in stable diffusion 3, we re-design the weighting to make it focus a bit more in the middle region.

To be continued... (09/26/2025)

